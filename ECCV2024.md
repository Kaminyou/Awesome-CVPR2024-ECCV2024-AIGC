# Awesome-ECCV2024-AIGC[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A Collection of Papers and Codes for ECCV2024 AIGC

**整理汇总下今年ECCV AIGC相关的论文和代码，具体如下。**

**欢迎star，fork和PR~**

**Please feel free to star, fork or PR if helpful~**

# **参考或转载请注明出处**

ECCV2024官网：https://eccv.ecva.net/

ECCV接收论文列表：https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers

ECCV完整论文库：https://openaccess.thecvf.com/CVPR2024

开会时间：2024年9月29日-10月4日

论文接收公布时间：2024年

**【Contents】**

- [1.图像生成(Image Generation/Image Synthesis)](#1.图像生成)
- [2.图像编辑（Image Editing)](#2.图像编辑)
- [3.视频生成(Video Generation/Image Synthesis)](#3.视频生成)
- [4.视频编辑(Video Editing)](#4.视频编辑)
- [5.3D生成(3D Generation/3D Synthesis)](#5.3D生成)
- [6.3D编辑(3D Editing)](#6.3D编辑)
- [7.多模态大语言模型(Multi-Modal Large Language Model)](#7.大语言模型)
- [8.其他多任务(Others)](#8.其他)

<a name="1.图像生成"></a>

# 1.图像生成(Image Generation/Image Synthesis)

### Accelerating Diffusion Sampling with Optimized Time Steps

- Paper: https://arxiv.org/abs/2402.17376
- Code: https://github.com/scxue/DM-NonUniform

### BeyondScene: Higher-Resolution Human-Centric Scene Generation With Pretrained Diffusion

- Paper: https://arxiv.org/abs/2404.04544
- Code: https://github.com/gwang-kim/BeyondScene

### DiffiT: Diffusion Vision Transformers for Image Generation

- Paper: https://arxiv.org/abs/2312.02139
- Code: https://github.com/NVlabs/DiffiT

### Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse Mixture-of-Experts

- Paper: https://arxiv.org/abs/2403.09176
- Code: https://github.com/byeongjun-park/Switch-DiT
  

<a name="2.图像编辑"></a>

# 2.图像编辑(Image Editing)


### A Task is Worth One Word: Learning with Task Prompts for High-Quality Versatile Image Inpainting

- Paper: https://arxiv.org/abs/2312.03594
- Code: https://github.com/open-mmlab/PowerPaint

### BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion

- Paper: https://arxiv.org/abs/2403.06976
- Code: https://github.com/TencentARC/BrushNet

### TinyBeauty: Toward Tiny and High-quality Facial Makeup with Data Amplify Learning

- Paper: https://arxiv.org/abs/2403.15033
- Code: https://github.com/TinyBeauty/TinyBeauty

<a name="3.视频生成"></a>

# 3.视频生成(Video Generation/Video Synthesis)

### Audio-Synchronized Visual Animation

- Paper: https://arxiv.org/abs/2403.05659
- Code: https://github.com/lzhangbj/ASVA
  
### EDTalk: Efficient Disentanglement for Emotional Talking Head Synthesis

- Paper: https://arxiv.org/abs/2404.01647
- Code: https://github.com/tanshuai0219/EDTalk
  
### MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model

- Paper: https://arxiv.org/abs/2405.20222
- Code: https://github.com/MyNiuuu/MOFA-Video
  


  
<a name="4.视频编辑"></a>

# 4.视频编辑(Video Editing)

### DragAnything: Motion Control for Anything using Entity Representation

- Paper: https://arxiv.org/abs/2403.07420
- Code: https://github.com/showlab/DragAnything




<a name="5.3D生成"></a>

# 5.3D生成(3D Generation/3D Synthesis)

### EchoScene: Indoor Scene Generation via Information Echo over Scene Graph Diffusion

- Paper: https://arxiv.org/abs/2405.00915
- Code: https://github.com/ymxlzgy/echoscene
  
### Motion Mamba: Efficient and Long Sequence Motion Generation with Hierarchical and Bidirectional Selective SSM

- Paper: https://arxiv.org/abs/2403.07487
- Code: https://github.com/steve-zeyu-zhang/MotionMamba
  
### ParCo: Part-Coordinating Text-to-Motion Synthesis

- Paper: https://arxiv.org/abs/2403.18512
- Code: https://github.com/qrzou/ParCo




<a name="6.3D编辑"></a>

# 6.3D编辑(3D Editing)






<a name="7.大语言模型"></a>

# 7.多模态大语言模型(Multi-Modal Large Language Models)

### ControlCap: Controllable Region-level Captioning

- Paper: https://arxiv.org/abs/2401.17910
- Code: https://github.com/callsys/ControlCap
  
### DriveLM: Driving with Graph Visual Question Answering

- Paper: https://arxiv.org/abs/2312.14150
- Code: https://github.com/OpenDriveLab/DriveLM

### Elysium: Exploring Object-level Perception in Videos via MLLM

- Paper: https://arxiv.org/abs/2403.16558
- Code: https://github.com/Hon-Wong/Elysium
  
### GiT: Towards Generalist Vision Transformer through Universal Language Interface

- Paper: https://arxiv.org/abs/2403.09394
- Code: https://github.com/Haiyang-W/GiT

### How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs

- Paper: https://arxiv.org/abs/2311.16101
- Code: https://github.com/UCSC-VLAA/vllm-safety-benchmark

### MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?

- Paper: https://arxiv.org/abs/2403.14624
- Code: https://github.com/ZrrSkywalker/MathVerse

### ShareGPT4V: Improving Large Multi-Modal Models with Better Captions

- Paper: https://arxiv.org/abs/2311.12793
- Code: https://github.com/ShareGPT4Omni/ShareGPT4V
  
### UniIR: Training and Benchmarking Universal Multimodal Information Retrievers

- Paper: https://arxiv.org/abs/2311.17136
- Code: https://github.com/TIGER-AI-Lab/UniIR


<a name="8.其他"></a>

# 8.其他任务(Others)





<font color=red size=5>持续更新~</font>

# 参考


# 相关整理

- [Awesome-CVPR2024-AIGC](https://github.com/Kobaayyy/Awesome-CVPR2024-AIGC/blob/main/CVPR2024.md)
- [Awesome-AIGC-Research-Groups](https://github.com/Kobaayyy/Awesome-AIGC-Research-Groups)
- [Awesome-Low-Level-Vision-Research-Groups](https://github.com/Kobaayyy/Awesome-Low-Level-Vision-Research-Groups)
- [Awesome-CVPR2024-CVPR2021-CVPR2020-Low-Level-Vision](https://github.com/Kobaayyy/Awesome-CVPR2024-CVPR2021-CVPR2020-Low-Level-Vision)
- [Awesome-ECCV2020-Low-Level-Vision](https://github.com/Kobaayyy/Awesome-ECCV2020-Low-Level-Vision)
